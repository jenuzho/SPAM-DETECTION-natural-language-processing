{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br>\n",
                "\n",
                "<br>\n",
                "\n",
                "<br>\n",
                "\n",
                "# ðŸ‘¾ **SPAM LINK DETECTION SYSTEM** ðŸ‘¾"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**NATURAL LANGUAGE PROCESSING**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br>\n",
                "\n",
                "## **INDEX**\n",
                "\n",
                "- **STEP 1: PROBLEM DEFINITION AND DATA COLLECTION**\n",
                "- **STEP 2: DATA EXPLORATION AND CLEANING**\n",
                "- **STEP 3: DATA PROCESSING**\n",
                "- **STEP 4: MODEL DEVELOPMENT: SUPPORT VECTOR MACHINE (SVM)**\n",
                "- **STEP 5: MODEL OPTIMIZATION**\n",
                "- **STEP 6: MODEL DEPLOYMENT AND SAVING**\n",
                "- **STEP 7: CONCLUSION**"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br>\n",
                "\n",
                "### **STEP 1: PROBLEM DEFINITION AND DATA COLLECTION**\n",
                "\n",
                "- 1.1. Define the problem\n",
                "- 1.2. Library Importing\n",
                "- 1.3. Data Collection"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "**1.1. PROBLEM DEFINITION**\n",
                "\n",
                "The increasing volume of web pages created daily has brought a proportional rise in spam and malicious URLs. These URLs often pose threats like phishing, malware, and other forms of cyber-attacks. The goal of this project is to create a **Spam Link Detection System** that can identify whether a URL is spam or legitimate based on its structure. By analyzing the patterns within URLs, we aim to automate this detection process, reducing the need for manual review and improving online security.\n",
                "\n",
                "<br>\n",
                "\n",
                "**What is Natural Language Processing (NLP)?**\n",
                "**Natural Language Processing (NLP)** is a branch of Artificial Intelligence (AI) that focuses on the interaction between computers and human language. **NLP** techniques enable machines to read, understand, and derive meaning from text data. \n",
                "\n",
                "In this project, URLs are treated as a form of text data, allowing us to leverage NLP techniques like tokenization, stopword removal, and lemmatization to preprocess and extract meaningful patterns from them.\n",
                "\n",
                "<br>\n",
                "\n",
                "**Data Processing**\n",
                "\n",
                "In relation to this project, **data processing** involves transforming raw URLs into a format suitable for machine learning models. This includes:\n",
                "- **Tokenization:** Breaking URLs into smaller components based on punctuation or special characters.  \n",
                "- **Stopword Removal:** Eliminating common yet uninformative words like \"www\" or \"http.\"  \n",
                "- **Lemmatization/Stemming:** Reducing words to their base or root forms.  \n",
                "\n",
                "These steps help highlight the key elements of URLs that are indicative of spam, ensuring that our model focuses on the most relevant features.\n",
                "\n",
                "<br>\n",
                "\n",
                "**Methodology: SUPPORT VECTOR MACHINE (SVM)**\n",
                "\n",
                "The **Support Vector Machine (SVM)** is a supervised learning algorithm widely used for classification problems. SVM works by finding the hyperplane that best separates data points into different classes. For this project:\n",
                "- We will use an **initial SVM model** with default parameters to classify URLs as spam or legitimate.  \n",
                "- **Hyperparameter optimization** will follow, refining the model for improved performance.  \n",
                "- The final model will be saved and deployed for real-world application, enabling automated spam detection.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br>\n",
                "\n",
                "**1.2. LIBRARY IMPORTING**"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "[nltk_data] Downloading package stopwords to\n",
                        "[nltk_data]     C:\\Users\\Jen\\AppData\\Roaming\\nltk_data...\n",
                        "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
                        "[nltk_data] Downloading package wordnet to\n",
                        "[nltk_data]     C:\\Users\\Jen\\AppData\\Roaming\\nltk_data...\n"
                    ]
                },
                {
                    "data": {
                        "text/plain": [
                            "True"
                        ]
                    },
                    "execution_count": 2,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "import nltk\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
                "from sklearn.svm import SVC \n",
                "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
                "import joblib\n",
                "import os \n",
                "import re  # For working with regular expressions (e.g., to split URLs)\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
                "\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "#nltk.download('punkt') hemos optado por \"re\" para la tokenizaciÃ³n.\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "<br>\n",
                "\n",
                "**1.3. DATA COLLECTION** "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Dataset loaded successfully.\n",
                        "The dataset has 2999 rows and 2 columns.\n",
                        "\n",
                        "First 5 rows of the dataset:\n",
                        "                                                 url  is_spam\n",
                        "0  https://briefingday.us8.list-manage.com/unsubs...     True\n",
                        "1                             https://www.hvper.com/     True\n",
                        "2                 https://briefingday.com/m/v4n3i4f3     True\n",
                        "3   https://briefingday.com/n/20200618/m#commentform    False\n",
                        "4                        https://briefingday.com/fan     True\n",
                        "\n",
                        "Missing values summary:\n",
                        "url        0\n",
                        "is_spam    0\n",
                        "dtype: int64\n",
                        "\n",
                        "Dataset columns:\n",
                        "Index(['url', 'is_spam'], dtype='object')\n"
                    ]
                }
            ],
            "source": [
                "\n",
                "def load_dataset(url, required_columns=None):\n",
                "    \"\"\"\n",
                "    Load a dataset from a given URL and validate its structure.\n",
                "    \n",
                "    Parameters:\n",
                "    - url (str): The URL to the CSV dataset.\n",
                "    - required_columns (list): List of required column names (optional).\n",
                "    \n",
                "    Returns:\n",
                "    - pd.DataFrame or None: The loaded dataset if successful, otherwise None.\n",
                "    \"\"\"\n",
                "    try:\n",
                "        # Load the dataset from the URL\n",
                "        data = pd.read_csv(url)\n",
                "\n",
                "        # Display an initial summary of the dataset\n",
                "        print(\"Dataset loaded successfully.\")\n",
                "        print(f\"The dataset has {data.shape[0]} rows and {data.shape[1]} columns.\\n\")\n",
                "        print(\"First 5 rows of the dataset:\")\n",
                "        print(data.head())\n",
                "\n",
                "        # Check for required columns if specified\n",
                "        if required_columns:\n",
                "            missing_columns = [col for col in required_columns if col not in data.columns]\n",
                "            if missing_columns:\n",
                "                print(f\"Error: The dataset is missing the following required columns: {missing_columns}\")\n",
                "                return None\n",
                "        \n",
                "        # Display missing values summary\n",
                "        print(\"\\nMissing values summary:\")\n",
                "        print(data.isnull().sum())\n",
                "        \n",
                "        return data\n",
                "\n",
                "    except Exception as e:\n",
                "        # Handle any errors that occur during loading\n",
                "        print(f\"An error occurred while loading the dataset: {e}\")\n",
                "        return None\n",
                "\n",
                "# Dataset URL\n",
                "url = \"https://raw.githubusercontent.com/4GeeksAcademy/NLP-project-tutorial/main/url_spam.csv\"\n",
                "\n",
                "# Function call with validation for 'url' and 'is_spam' columns\n",
                "required_columns = ['url', 'is_spam']\n",
                "dataset = load_dataset(url, required_columns)\n",
                "\n",
                "# Additional verification\n",
                "if dataset is not None:\n",
                "    print(\"\\nDataset columns:\")\n",
                "    print(dataset.columns)\n"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Your code here"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
